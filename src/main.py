from nn.nn import NeuralNetwork
import glob
import json
import os
import random
import time



LANG_LIST="pl,en".split(",")
ALPHABET="abcdefghijklmnopqrstuvwxyz"
MAX_LETTERS=12
WORDS={}



def encode_word(w):
	e=[0]*len(ALPHABET)*MAX_LETTERS
	i=0
	for c in w[:MAX_LETTERS]:
		e[i*len(ALPHABET)+ALPHABET.index(c)]=1
		i+=1
	return e
def encode_lang(l):
	e=[0]*len(LANG_LIST)
	e[LANG_LIST.index(l)]=1
	return e



for l in LANG_LIST:
	with open(f"./data/{l}.txt","r") as f:
		WORDS[l]=f.read().lower().split("\n")



def batch(N):
	B=[]
	for l in LANG_LIST:
		wl=WORDS[l][:]
		random.shuffle(wl)
		for w in wl[:N]:
			B+=[[encode_word(w),encode_lang(l)]]
	random.shuffle(B)
	return B



def train(NN,t,BS,log=True,S=0):
	l=-1
	st=time.time()
	for i in range(S*int(t/10000),t):
		d=batch(BS)
		if (log==True and int(i/t*10000)>l):
			l=int(i/t*10000)
			print(f"{l/100}% complete... ({int((time.time()-st)*100)/100}s) Acc={NN.test(batch(1000),log=False)}%")
			st=time.time()
			open(f"./json/NN-data-{l}.json","w").write(json.dumps(NN.toJSON(),indent=4,sort_keys=True))
		for k in d:
			NN.train(k[0],k[1])



def transform_s(S):
	S="".join([e for e in list(str(S).lower()) if ord(e)<127])
	N=""
	for k in S:
		if (k==" " or k in ALPHABET):
			N+=k
	return N



def predict(W):
	o=NN.predict(encode_word(W))
	s=sum(o)
	i=0
	for k in o:
		o[i]=k/s
		i+=1
	i=0
	for k in LANG_LIST:
		print(f"{k.title()} \u2012 {int(o[i]*10000)/100}")
		i+=1



def predict_sentence(S):
	S=transform_s(S)
	a=[0]*len(LANG_LIST)
	for W in S.split(" "):
		o=NN.predict(encode_word(W))
		s=sum(o)
		i=0
		for k in o:
			a[i]+=k/s
			i+=1
	t=sum(a)
	for i in range(0,len(a)):
		a[i]/=t
	i=0
	for k in LANG_LIST:
		print(f"{k.title()} \u2012 {int(a[i]*10000)/100}")
		i+=1



def grammar_correction(SW,NN):
	S=transform_s(SW)
	a=[0]*len(LANG_LIST)
	d={}
	j=0
	for W in S.split(" "):
		o=NN.predict(encode_word(W))
		s=sum(o)
		i=0
		d[j]=[]
		for k in o:
			a[i]+=k/s
			d[j]+=[k/s]
			i+=1
		j+=1
	t=sum(a)
	for i in range(0,len(a)):
		a[i]/=t
	L=a.index(max(a))
	ERR=[]
	for k in d.keys():
		w=d[k]
		if (w[L]<max(w)):
			ERR+=[[k,w.index(max(w))]]
	s=SW+"\n"
	i=0
	for k in ERR:
		off=len(" ".join(SW.split(" ")[:k[0]]))+1-(0 if i==0 else len(" ".join(SW.split(" ")[:ERR[i-1][0]+1])))
		l=len(SW.split(" ")[k[0]])
		s+=" "*off+"^"*l
		i+=1
	print(s)



def train_mode():
	fp=glob.glob("./json/*.json")[-1]
	if (os.path.isfile(fp)):
		with open(f,"r") as f:
			NN=NeuralNetwork(json.loads(f.read()))
	else:
		NN=NeuralNetwork(len(ALPHABET)*MAX_LETTERS,[MAX_LETTERS+2],len(LANG_LIST),lr=0.0075)
	NN.lr=0.0055
	train(NN,100_000,10,S=int(f.rsplit("-",1)[1].split(".")[0]))
	with open("./json/NN-data-FULL.json","w") as f:
		f.write(json.dumps(NN.toJSON(),indent=4,sort_keys=True))



def test_mode():
	fp=glob.glob("./json/*.json")[-1]
	with open(fp,"r") as f:
		NN=NeuralNetwork(json.loads(f.read()))
	S="Hej lubiÄ™ you!"
	grammar_correction(S,NN)



# train_mode()
test_mode()
